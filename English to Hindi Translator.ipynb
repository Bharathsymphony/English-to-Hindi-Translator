{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqZo-dU3WIYm",
        "outputId": "1b0f5536-5a5c-4b85-a6db-b3070f817b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep  1 05:24:46 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P8    14W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#Checking if GPU is running or not\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[sentencepiece] sacrebleu -q"
      ],
      "metadata": {
        "id": "31Yf-u5UWZjP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Required Libraries\n",
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "vUUGPbFjWsjy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-hi\""
      ],
      "metadata": {
        "id": "5U-hhR6uWzib"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Dataset (Source: https://huggingface.co/datasets/cfilt/iitb-english-hindi)\n",
        "raw_datasets = load_dataset(\"cfilt/iitb-english-hindi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbHkrWjJW0ML",
        "outputId": "6db67db2-b19d-42bb-d0d0-9c45c4a1cf3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "raw_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kGzEvMwW4Ua",
        "outputId": "10620985-7ad7-4669-8e56-8366cabacd29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 1659083\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 520\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 2507\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2beDG4UMGZBy",
        "outputId": "ab0851c2-4b44-4a30-ce9f-1aaab34413c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'translation': {'en': 'Accerciser Accessibility Explorer',\n",
              "  'hi': 'एक्सेर्साइसर पहुंचनीयता अन्वेषक'}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['test'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPXCjdmIW76K",
        "outputId": "e052e4dd-123d-4247-cf7d-de88cac5ff2f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'translation': {'en': \"As America's road planners struggle to find the cash to mend a crumbling highway system, many are beginning to see a solution in a little black box that fits neatly by the dashboard of your car.\",\n",
              "  'hi': 'जबकि अमेरिका के सड़क योजनाकार, ध्वस्त होते हुए हाईवे सिस्टम को सुधारने के लिए धन की कमी से जूझ रहे हैं, वहीं बहुत-से लोग इसका समाधान छोटे से ब्लैक बॉक्स में देख रहे हैं, जो आपकी कार के डैशबोर्ड पर सफ़ाई से फिट हो जाता है।'}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "_62mnKlx6n34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ijq1OvFXAHY",
        "outputId": "ece67ae4-f941-4335-9daa-cc8dfe06eee8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer([\"I had about a 30 minute demo just using this new headset\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUOMZeib7eYC",
        "outputId": "e5fb3fa3-fe44-485f-fc5f-2215bcaaa7cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[56, 154, 195, 19, 1671, 7336, 35914, 469, 1192, 90, 336, 1876, 8907, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"मझु ेसि र्फ ३० minute का demo मि ला था इस नयेheadset का इस्तमे ाल करने के लिए\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m2Jp57H78N6",
        "outputId": "ff3d15dd-88e0-4a48-ff99-6d86d044d4a2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[4095, 14034, 3625, 44, 174, 18943, 3353, 14921, 6785, 3383, 39169, 24, 3947, 363, 818, 3245, 1754, 82, 89, 4075, 2326, 48677, 8907, 24, 89, 9370, 9436, 44, 17182, 91, 6, 39, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenzing the English and Hindi words\n",
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "source_lang = \"en\"\n",
        "target_lang = \"hi\"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "-7N2YMVIXTLb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_function(raw_datasets[\"train\"][:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75l8HXapXXc7",
        "outputId": "f66644b9-57a6-457b-8b24-9dc95026327d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[3872, 85, 2501, 132, 15441, 36398, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]], 'labels': [[63, 2025, 18, 16155, 346, 20311, 24, 2279, 679, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "nWU4dU93Xd2T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoEgazPiXg-E",
        "outputId": "e1e544f9-5b73-47a2-a201-6e0965b2be11"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-hi.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Hyper Parameter\n",
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "num_train_epochs = 10\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
        "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
      ],
      "metadata": {
        "id": "C-ANchYaXkkb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqRZitzkO0o1",
        "outputId": "07777d8b-8d1e-4dca-ad29-aff5351307f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 1659083\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting the Dataset to Train and Test data\n",
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets['test'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "-wmNchvcXyry"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declaring the optimizer for improving the accuracy and reduce the loss\n",
        "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "BTrhB1WjX7dJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "KqTOmFU4-pdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset, validation_data=validation_dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxtbaVpYYCGF",
        "outputId": "a26170bd-3c5b-455f-f037-a627ec71b8cb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "156/156 [==============================] - 94s 375ms/step - loss: 3.7551 - val_loss: 3.9527\n",
            "Epoch 2/10\n",
            "156/156 [==============================] - 49s 312ms/step - loss: 3.3230 - val_loss: 3.8702\n",
            "Epoch 3/10\n",
            "156/156 [==============================] - 51s 325ms/step - loss: 3.0182 - val_loss: 3.8317\n",
            "Epoch 4/10\n",
            "156/156 [==============================] - 50s 323ms/step - loss: 2.7760 - val_loss: 3.8207\n",
            "Epoch 5/10\n",
            "156/156 [==============================] - 50s 320ms/step - loss: 2.5713 - val_loss: 3.8223\n",
            "Epoch 6/10\n",
            "156/156 [==============================] - 49s 314ms/step - loss: 2.3807 - val_loss: 3.8225\n",
            "Epoch 7/10\n",
            "156/156 [==============================] - 49s 316ms/step - loss: 2.2086 - val_loss: 3.8284\n",
            "Epoch 8/10\n",
            "156/156 [==============================] - 50s 320ms/step - loss: 2.0565 - val_loss: 3.8544\n",
            "Epoch 9/10\n",
            "156/156 [==============================] - 51s 328ms/step - loss: 1.9127 - val_loss: 3.8658\n",
            "Epoch 10/10\n",
            "156/156 [==============================] - 51s 324ms/step - loss: 1.7789 - val_loss: 3.8789\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ee0130dabc0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"tf_model/\")"
      ],
      "metadata": {
        "id": "w5IgHS_xYEWG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing"
      ],
      "metadata": {
        "id": "v5LOhh5k-26-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "metadata": {
        "id": "zo3O58hLHMAJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"tf_model/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEPMgmZ_YGEK",
        "outputId": "5dd105d2-1e5c-4ee1-fd31-00e34e4620e2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at tf_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train']['translation'][1]['en']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pws56iiNXGjB",
        "outputId": "f12bf6cc-7f46-4566-f8e3-13af3adcdcf2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Accerciser Accessibility Explorer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_input=raw_datasets['train']['translation'][1]['en']\n",
        "tokenized = tokenizer([pred_input], return_tensors='np')\n",
        "out = model.generate(**tokenized, max_length=128)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmHC23BgGli7",
        "outputId": "439b661b-28e8-427c-a163-85016e930264"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[61949 26618 16155   346 33383     0 61949]], shape=(1, 7), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    prediction=tokenizer.decode(out[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "-DEXiHveGlZ-"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input value\n",
        "raw_datasets['train']['translation'][1]['en']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vQE-Wt5bYlXg",
        "outputId": "0f2f1890-17aa-4bfc-cc12-5ccc7f0aa43f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Accerciser Accessibility Explorer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Orginal values\n",
        "raw_datasets['train']['translation'][1]['hi']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Sc1OCST0YPwa",
        "outputId": "097de66a-cfad-490f-b61d-ab3c808e01ce"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'एक्सेर्साइसर पहुंचनीयता अन्वेषक'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted Value\n",
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rEzk4V6QGk9O",
        "outputId": "1542bed0-94cd-4010-88ad-ad94fc0332bd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'एक्सेर्साइसर पहुंचनीयता अन्वेषक'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing with New Inputs"
      ],
      "metadata": {
        "id": "zWT_0uATZ8Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input 1\n",
        "input_text  = \"Definitely share your feedback in the comment section.\"\n",
        "\n",
        "tokenized = tokenizer([input_text], return_tensors='np')\n",
        "out = model.generate(**tokenized, max_length=128)\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twZ9CtcKYKep",
        "outputId": "e11d3066-537a-4e80-e1a6-e6dd4fecba2f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "टिप्पणी खंड में निश्चित रूप से अपनी प्रतिक्रिया साझा करें।\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input 2\n",
        "input_text  = \"So even if it's a big video, I will clearly mention all the products.\"\n",
        "\n",
        "tokenized = tokenizer([input_text], return_tensors='np')\n",
        "out = model.generate(**tokenized, max_length=128)\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "_zacpzSRYead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d8acbf4-7641-4ebb-c25c-4cfb71c1b328"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "तो यह एक बड़ा वीडियो है, तो भी मैं सभी उत्पादों का स्पष्ट रूप से उल्लेख करेंगे।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input 3\n",
        "input_text  = \"I was waiting for my bag.\"\n",
        "\n",
        "tokenized = tokenizer([input_text], return_tensors='np')\n",
        "out = model.generate(**tokenized, max_length=128)\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "ib-EQc_cYf2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa89849-8631-49c9-e1a0-835e191edd7c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "मैं अपने बैग के लिए प्रतीक्षा कर रहा था।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MGgQ1UmB0lGg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}